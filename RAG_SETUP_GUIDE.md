# ğŸš€ RAG Setup & Testing Guide

## âœ… What's Been Implemented

The full RAG (Retrieval-Augmented Generation) system is now complete:

1. âœ… Text processing utilities (HTML cleaning, chunking)
2. âœ… OpenAI embeddings service with batching
3. âœ… Pinecone vector database client
4. âœ… Data ingestion API endpoints
5. âœ… RAG query orchestration with streaming
6. âœ… Updated chat API with real RAG pipeline
7. âœ… Frontend with streaming responses and citations

---

## ğŸ”§ Setup Instructions

### Step 1: Configure Environment Variables

Create `.env.local` in the `chat-ui` directory:

```bash
# Required: OpenAI API Key
OPENAI_API_KEY=sk-proj-...

# Required: Pinecone API Key
PINECONE_API_KEY=...

# Optional: Pinecone Configuration
PINECONE_ENVIRONMENT=us-east-1
PINECONE_INDEX_NAME=wordpress-archive

# Optional: Processing Configuration
CHUNK_SIZE=800
CHUNK_OVERLAP=200
EMBEDDING_BATCH_SIZE=100
```

**Get your API keys:**
- OpenAI: https://platform.openai.com/api-keys
- Pinecone: https://app.pinecone.io/

### Step 2: Install Dependencies (Already Done)

Dependencies installed:
- `openai` - OpenAI SDK
- `@pinecone-database/pinecone` - Pinecone client
- `ai` - Vercel AI SDK
- `zod` - Schema validation
- `papaparse` - CSV parsing

### Step 3: Start Development Server

```bash
npm run dev
```

Server will start at: **http://localhost:3001**

---

## ğŸ“Š Data Ingestion Workflow

### Phase 1: Check System Health

```bash
# Check if API keys are configured
curl http://localhost:3001/api/chat
```

Expected response:
```json
{
  "status": "not_ready",
  "config": {
    "openai": "configured",
    "pinecone": "configured",
    "index": "not_created"
  }
}
```

### Phase 2: Initialize Pinecone Index

```bash
# Create the Pinecone index (3072-dim, cosine similarity)
curl -X POST http://localhost:3001/api/ingest/init
```

Expected response:
```json
{
  "success": true,
  "message": "Index 'wordpress-archive' created successfully",
  "indexName": "wordpress-archive",
  "stats": { ... }
}
```

**Note:** This only needs to be done once. The index will persist in Pinecone.

### Phase 3: Ingest Sample Data (Test with 100 articles)

```bash
# Ingest first 100 articles from wordpress_posts.csv
curl -X POST "http://localhost:3001/api/ingest?batch=0&size=100"
```

Expected response:
```json
{
  "success": true,
  "message": "Successfully processed batch 0",
  "stats": {
    "articlesProcessed": 100,
    "chunksGenerated": 450,
    "embeddingsCreated": 450,
    "vectorsUpserted": 450,
    "totalTokens": 125000,
    "estimatedCost": 0.016
  },
  "duration": "45.23s",
  "progress": {
    "processed": 100,
    "remaining": 540809,
    "percentComplete": "0.02%"
  }
}
```

**Processing Time:**
- 100 articles: ~45 seconds
- 1,000 articles: ~7 minutes
- Full dataset (540K): ~12-24 hours

**Cost Estimates:**
- 100 articles: ~$0.02
- 1,000 articles: ~$0.15
- Full dataset: ~$50-100 (embeddings only)

### Phase 4: Test Chat Interface

1. Open **http://localhost:3001**
2. You should see the Hero Chat interface
3. Try a query: **"What articles discuss Stoicism?"**
4. Watch the streaming response appear token-by-token
5. Check citations below the response

---

## ğŸ§ª Testing Checklist

### âœ… Environment Configuration
```bash
# Verify .env.local exists
[ -f .env.local ] && echo "âœ… .env.local found" || echo "âŒ Create .env.local"

# Check if variables are set
grep -q "OPENAI_API_KEY=" .env.local && echo "âœ… OpenAI key configured" || echo "âŒ Add OPENAI_API_KEY"
grep -q "PINECONE_API_KEY=" .env.local && echo "âœ… Pinecone key configured" || echo "âŒ Add PINECONE_API_KEY"
```

### âœ… API Health Checks
```bash
# 1. Check chat API status
curl http://localhost:3001/api/chat | jq '.config'

# 2. Check ingest API status
curl http://localhost:3001/api/ingest | jq '.csvFile'

# 3. Verify index exists
curl http://localhost:3001/api/ingest/init | jq '.exists'
```

### âœ… Sample Data Ingestion
```bash
# Ingest 10 articles for quick testing
curl -X POST "http://localhost:3001/api/ingest?batch=0&size=10" | jq '.stats'
```

### âœ… Chat Functionality

Test queries to try:
1. **"What have you written about discipline?"**
2. **"Find podcast episodes on productivity"**
3. **"Articles about Theodore Roosevelt"**
4. **"What fitness routines do you recommend?"**

Expected behavior:
- âœ… Streaming response appears token-by-token
- âœ… Citations display below the response
- âœ… Source URLs are clickable
- âœ… Relevance scores shown (e.g., "85% match")
- âœ… Conversation history maintained

---

## ğŸ“ Project Structure

```
chat-ui/
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ text-processing.ts      âœ… HTML cleaning, chunking
â”‚   â”œâ”€â”€ embeddings.ts            âœ… OpenAI embeddings with batching
â”‚   â”œâ”€â”€ pinecone.ts              âœ… Vector DB operations
â”‚   â”œâ”€â”€ rag.ts                   âœ… RAG orchestration + streaming
â”‚   â””â”€â”€ utils.ts                 âœ… Utility functions
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ chat/route.ts        âœ… Streaming chat with RAG
â”‚   â”‚   â””â”€â”€ ingest/
â”‚   â”‚       â”œâ”€â”€ init/route.ts    âœ… Index initialization
â”‚   â”‚       â””â”€â”€ route.ts         âœ… Data ingestion
â”‚   â”œâ”€â”€ layout.tsx               âœ… Root layout
â”‚   â”œâ”€â”€ page.tsx                 âœ… Main page
â”‚   â””â”€â”€ globals.css              âœ… Global styles
â”‚
â”œâ”€â”€ components/ui/
â”‚   â”œâ”€â”€ ruixen-moon-chat.tsx     âœ… Chat UI with streaming + citations
â”‚   â”œâ”€â”€ button.tsx               âœ… shadcn Button
â”‚   â””â”€â”€ textarea.tsx             âœ… shadcn Textarea
â”‚
â”œâ”€â”€ .env.example                 âœ… Environment template
â”œâ”€â”€ .env.local                   âš ï¸  YOU CREATE THIS
â””â”€â”€ package.json                 âœ… Dependencies installed
```

---

## ğŸ”¥ Full Dataset Ingestion (Production)

Once you've tested with sample data, ingest the full dataset:

### Option 1: Batch-by-Batch (Manual)

```bash
# Process batches of 100 articles
for i in {0..5408}; do
  echo "Processing batch $i..."
  curl -X POST "http://localhost:3001/api/ingest?batch=$i&size=100"
  sleep 2  # Rate limiting
done
```

### Option 2: Large Batches

```bash
# Process 1000 articles at a time (faster, but more expensive)
for i in {0..540}; do
  curl -X POST "http://localhost:3001/api/ingest?batch=$i&size=1000"
done
```

### Option 3: Node Script (Recommended)

Create `scripts/ingest-all.js`:

```javascript
const TOTAL_BATCHES = 5409; // 540,909 articles / 100
const BATCH_SIZE = 100;
const CONCURRENCY = 3; // Process 3 batches in parallel

async function ingestBatch(batchNumber) {
  const response = await fetch(
    `http://localhost:3001/api/ingest?batch=${batchNumber}&size=${BATCH_SIZE}`,
    { method: 'POST' }
  );
  return await response.json();
}

async function ingestAll() {
  for (let i = 0; i < TOTAL_BATCHES; i += CONCURRENCY) {
    const promises = [];
    for (let j = 0; j < CONCURRENCY && i + j < TOTAL_BATCHES; j++) {
      promises.push(ingestBatch(i + j));
    }
    const results = await Promise.all(promises);
    console.log(`Processed batches ${i} to ${i + CONCURRENCY - 1}`);
  }
}

ingestAll().then(() => console.log('Done!'));
```

Run:
```bash
node scripts/ingest-all.js
```

**Estimated time:** 12-24 hours for full dataset

---

## ğŸ› Troubleshooting

### Issue: "Knowledge base not initialized"

**Solution:**
```bash
curl -X POST http://localhost:3001/api/ingest/init
```

### Issue: "OPENAI_API_KEY is not set"

**Solution:**
1. Create `.env.local` in `chat-ui/` directory
2. Add: `OPENAI_API_KEY=sk-proj-...`
3. Restart dev server

### Issue: "Failed to query vectors"

**Possible causes:**
- Pinecone index doesn't exist â†’ Run `/api/ingest/init`
- No data ingested yet â†’ Run `/api/ingest?batch=0&size=10`
- Wrong index name â†’ Check `PINECONE_INDEX_NAME` in `.env.local`

### Issue: Slow responses

**Solutions:**
- Reduce `topK` in `lib/rag.ts` (default: 5)
- Use GPT-4o-mini instead of GPT-4o for faster responses
- Check Pinecone region (use same region as your location)

### Issue: Citations not appearing

**Causes:**
- Headers might be blocked by browser
- Check browser console for errors
- Verify citations are in API response headers

---

## ğŸ“Š Monitoring

### Check Index Stats
```bash
curl http://localhost:3001/api/ingest/init | jq '.stats'
```

Response:
```json
{
  "namespaces": {},
  "dimension": 3072,
  "indexFullness": 0.2,
  "totalVectorCount": 45000
}
```

### Monitor Costs

Track OpenAI usage:
- Embeddings: ~$0.00013 per 1K tokens
- GPT-4o: ~$0.03 per 1K tokens (input), ~$0.06 (output)

Example for 100 articles (~125K tokens):
- Embeddings: $0.016
- Queries (10 chats): ~$0.05
- **Total: ~$0.07**

---

## âœ… Success Criteria

Your RAG system is working correctly if:

1. âœ… Health check shows all systems "ready"
2. âœ… Sample ingestion completes without errors
3. âœ… Chat returns streaming responses
4. âœ… Citations display with valid URLs
5. âœ… Responses are grounded in retrieved context
6. âœ… Conversation history maintained across messages
7. âœ… No "I don't know" when data exists in the archive

---

## ğŸš€ Next Steps

After confirming everything works:

1. **Ingest full dataset** (optional - start with 1000 articles for testing)
2. **Fine-tune parameters**:
   - Adjust `topK` (number of chunks retrieved)
   - Tweak `temperature` (0.3 = factual, 0.7 = creative)
   - Modify chunk size/overlap in text-processing.ts
3. **Deploy to production**:
   - Deploy to Vercel, Netlify, or your hosting platform
   - Set environment variables in hosting dashboard
   - Pinecone data persists - no need to re-ingest
4. **Add features**:
   - User authentication
   - Conversation persistence (database)
   - Export chats as PDF/Markdown
   - Advanced search filters (date range, categories)

---

## ğŸ†˜ Need Help?

- Check logs in terminal (`npm run dev`)
- Test API endpoints with curl
- Verify environment variables are set
- Check Pinecone dashboard: https://app.pinecone.io/
- Review OpenAI API usage: https://platform.openai.com/usage

**Common pitfalls:**
- Missing `.env.local` file
- Wrong working directory (must be in `chat-ui/`)
- CSV file not in parent directory
- API keys have insufficient permissions

---

**Your RAG system is ready!** ğŸ‰

Test with sample data first, then scale to the full dataset when ready.
